{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop ML model with MLflow and deploy to Kubernetes\n",
    "\n",
    "This a notbook which summarise the steps done in the tutorial [Develop ML model with MLflow and deploy to Kubernetes](https://mlflow.org/docs/latest/deployment/deploy-model-to-kubernetes/tutorial.html#develop-ml-model-with-mlflow-and-deploy-to-kubernetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* PyENV already installed. Please check the [link section](#links)\n",
    "* Kind [Cluster](./README.md#cluster)\n",
    "* A python3.10 virtual environment\n",
    "* mflow on the python virtual environment\n",
    "* ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.client import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_metrics(pred, actual):\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(actual, pred))\n",
    "    mae = metrics.mean_absolute_error(actual, pred)\n",
    "    r2 = metrics.r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Load wine quality dataset\n",
    "X, y = datasets.load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a mflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Set th experiment name\n",
    "mlflow.set_experiment(\"wine-quality\")\n",
    "\n",
    "# Enable auto-logging to MLflow\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "\n",
    "# Start a run and train a model\n",
    "with mlflow.start_run(run_name=\"default-params\"):\n",
    "    lr = ElasticNet()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test)\n",
    "    metrics = eval_metrics(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the experiment\n",
    "\n",
    "Before running the mflow server ui on port 5000, we checkout if there is a process using port 5000\n",
    "\n",
    "In case you cannot access to mflow ui on URL [http://localhost:5000](http://localhost:5000). Please check out if there is a process using port 5000 by executing ```lsof -i :5000```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, socket\n",
    "\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "    sock.settimeout(5)\n",
    "    result = sock.connect_ex(('localhost',5000))\n",
    "\n",
    "if result == 0:\n",
    "    print('Server already running')\n",
    "else:\n",
    "    command = \"mlflow ui --port 5000\"\n",
    "    \n",
    "    process = subprocess.Popen(command, shell=True)\n",
    "    \n",
    "    print(f\"Started background process with PID: {process.pid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_MODEL_NAME = \"wine-quality\"\n",
    "ARTIFACT_PATH = \"wine-quality_model\"\n",
    "\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"logging-model\") as run:\n",
    "\n",
    "    lr = ElasticNet()\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"wine-quality\", \"Basic ElasticNet model for wine-quality\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        artifact_path=ARTIFACT_PATH,\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=REGISTERED_MODEL_NAME,\n",
    "    )\n",
    "    # set extra tags on the model\n",
    "    client = MlflowClient(mlflow.get_tracking_uri())\n",
    "    model_info = client.get_latest_versions(REGISTERED_MODEL_NAME)[0]\n",
    "    client.set_model_version_tag(\n",
    "        name=REGISTERED_MODEL_NAME,\n",
    "        version=model_info.version,\n",
    "        key='model',\n",
    "        value='ElasticNet'\n",
    "    )\n",
    "\n",
    "    print(f'Model Info: {model_info}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Serving Locally\n",
    "\n",
    "For this step we need to gather from the artifact section of the latest experiment run.\n",
    "\n",
    "![image](./image/Screenshot%20from%202024-05-20%2019-35-09.png)\n",
    "\n",
    "Then we can run the mlserver manually by executing the command below where the last runid is ```ef001c9ef0b3485999157d2ec3d5a2a8```:\n",
    "```bash\n",
    "mlflow models serve -m runs:/ef001c9ef0b3485999157d2ec3d5a2a8/wine-quality_model -p 1234 --enable-mlserver\n",
    "```\n",
    "\n",
    "Finally, querying the endpoing by using the curl command below:\n",
    "\n",
    "```bash\n",
    "curl -X POST -H \"Content-Type:application/json\" --data '{\"inputs\": [[14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]]}' http://127.0.0.1:1234/invocations\n",
    "```\n",
    "\n",
    "We will get the outcome similar to the one below:\n",
    "```bash\n",
    "{\"predictions\": [0.4230039055961139]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or running the next python cell run launch a mlserver process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, socket\n",
    "\n",
    "### Getting runID\n",
    "port  = 1234\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "    sock.settimeout(5)\n",
    "    result = sock.connect_ex(('localhost',port))\n",
    "\n",
    "if result == 0:\n",
    "    print('Server already running')\n",
    "else:\n",
    "    command = f\"mlflow models serve -m runs:/{run.info.run_id}/{ARTIFACT_PATH} -p {port} --enable-mlserver\"\n",
    "    \n",
    "    process = subprocess.Popen(command, shell=True)\n",
    "    \n",
    "    print(f'Executing: {command}')\n",
    "    print(f\"Started background process with PID: {process.pid} for model {run.info.run_id}/{ARTIFACT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will query the endpoint using the next python cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'http://localhost:{port}/invocations'\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data = {\"inputs\": [[14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]]}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url,headers=headers, json=data)\n",
    "    print(f'Status Code: {response.status_code}, Result: {response.json()}')\n",
    "except Exception as e:\n",
    "    print(f'ERROR!!: An expected error happened: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model to KServe\n",
    "\n",
    "#### Building a Docker Image\n",
    "\n",
    "```bash\n",
    "mlflow models build-docker -m runs:/[the_latest_run_id]/wine-quality_model -n [dockerhub_user]/mlflow-wine-classifier --enable-mlserver\n",
    "```\n",
    "\n",
    "#### Pushing the Docker Image\n",
    "```bash\n",
    "docker push [dockerhub_user]/mlflow-wine-classifier\n",
    "```\n",
    "\n",
    "#### Deploying a Inference Service\n",
    "\n",
    "We will deploy a Inference Service in our kubernetes cluster with KServer\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "### Creating a namespace mlflow-kserve-test\n",
    "kubectl create namespace mlflow-kserve-test\n",
    "\n",
    "### Creatinga Inference Service\n",
    "\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"mlflow-wine-classifier\"\n",
    "  namespace: \"mlflow-kserve-test\"\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: mlflow\n",
    "      protocolVersion: v2\n",
    "      storageUri: \"gs://mlflow-wine-classifier/wine-quality_model\"\n",
    "EOF\n",
    "\n",
    "```\n",
    "\n",
    "### Checking the Inference Service\n",
    "\n",
    "```bash\n",
    "watch kubectl -n mlflow-kserve-test get inferenceservice mlflow-wine-classifier\n",
    "```\n",
    "\n",
    "After some minutes the output expected will be something similar to the one below:\n",
    "```bash\n",
    "NAME                     URL                                                            READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                      AGE\n",
    "mlflow-wine-classifier   http://mlflow-wine-classifier.mlflow-kserve-test.example.com   True           100                              mlflow-wine-classifier-predictor-00001   2m27s\n",
    "```\n",
    "\n",
    "### Testing the deployment\n",
    "\n",
    "We will use a node IP to test out the deploy due to the model is exposed by an istio-ingressgatway loadBalancer\n",
    "\n",
    "Firslty, we wil lset up manually the bash variable INGRESS_HOST and INGRESS_PORT\n",
    "\n",
    "```bash\n",
    "export INGRESS_HOST=$(kubectl get nodes -o json | jq '.items[1].status.addresses[] | select(.type==\"InternalIP\") | .address' | sed 's/\"//g') # Assuming that there is at least one worker\n",
    "export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n",
    "echo \"Host: $INGRESS_HOST, Port: $INGRESS_PORT\"\n",
    "```\n",
    "\n",
    "\n",
    "Finally, using the file [test-input.json](./test-input.json), we will test out the inference endpoint by executing the bash snippet below:\n",
    "\n",
    "```bash\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice mlflow-wine-classifier -n mlflow-kserve-test -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -H \"Content-Type: application/json\" -d @./test-input.json \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/mlflow-wine-classifier:predict\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "### PyENV instalaltion\n",
    "* Easy guid to install PyENV in ubuntu, https://medium.com/@aashari/easy-to-follow-guide-of-how-to-install-pyenv-on-ubuntu-a3730af8d7f0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
